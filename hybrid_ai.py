#!/usr/bin/env python3
"""
HyperCLOVAX í•˜ì´ë¸Œë¦¬ë“œ AI
- ê¸°ë³¸: Ollama (qwen2.5:3b) - ë¹ ë¦„
- ì„ íƒ: HyperCLOVAX CPU - ëŠë¦¬ì§€ë§Œ ì •í™•
- ë§ˆì´í¬ ì²­ì·¨
- ë„êµ¬ ì‹¤í–‰
- ìê°€ í•™ìŠµ
"""

import os
import sys
import json
import time
import queue
import threading
import subprocess
import tempfile
import requests
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any, Callable
from dataclasses import dataclass, field
import warnings

warnings.filterwarnings("ignore")

# ì„¤ì •
OLLAMA_MODEL = "qwen2.5:3b"
OLLAMA_URL = "http://localhost:11434"
MEMORY_FILE = "/mnt/data/HyperCLOVAX-AGI/ai_memory.json"


@dataclass
class Memory:
    """AI ë©”ëª¨ë¦¬"""
    conversations: List[Dict] = field(default_factory=list)
    learnings: List[Dict] = field(default_factory=list)
    tools_used: List[Dict] = field(default_factory=list)

    def save(self):
        data = {
            "conversations": self.conversations[-50:],
            "learnings": self.learnings[-200:],
            "tools_used": self.tools_used[-100:],
            "updated": datetime.now().isoformat(),
        }
        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if os.path.exists(MEMORY_FILE):
            try:
                with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.conversations = data.get("conversations", [])
                self.learnings = data.get("learnings", [])
                self.tools_used = data.get("tools_used", [])
                print(f"ğŸ“š ë©”ëª¨ë¦¬ ë¡œë“œ: {len(self.learnings)}ê°œ í•™ìŠµ")
            except:
                pass

    def add_conversation(self, role: str, content: str):
        self.conversations.append({
            "role": role,
            "content": content,
            "time": datetime.now().isoformat()
        })

    def learn(self, category: str, content: str):
        self.learnings.append({
            "category": category,
            "content": content,
            "time": datetime.now().isoformat()
        })
        print(f"ğŸ’¡ í•™ìŠµë¨: [{category}] {content[:50]}...")

    def log_tool(self, tool: str, args: str, result: str):
        self.tools_used.append({
            "tool": tool,
            "args": args,
            "result": result[:500],
            "time": datetime.now().isoformat()
        })


class Tools:
    """ë„êµ¬ ëª¨ìŒ"""

    @staticmethod
    def bash(cmd: str) -> str:
        """Bash ëª…ë ¹ ì‹¤í–‰"""
        try:
            r = subprocess.run(
                cmd, shell=True,
                capture_output=True, text=True,
                timeout=30
            )
            return (r.stdout + r.stderr)[:2000] or "[ì™„ë£Œ]"
        except subprocess.TimeoutExpired:
            return "[íƒ€ì„ì•„ì›ƒ]"
        except Exception as e:
            return f"[ì˜¤ë¥˜: {e}]"

    @staticmethod
    def python(code: str) -> str:
        """Python ì‹¤í–‰"""
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                path = f.name
            r = subprocess.run(
                ['python3', path],
                capture_output=True, text=True,
                timeout=30
            )
            os.unlink(path)
            return (r.stdout + r.stderr)[:2000] or "[ì™„ë£Œ]"
        except Exception as e:
            return f"[ì˜¤ë¥˜: {e}]"

    @staticmethod
    def read_file(path: str) -> str:
        """íŒŒì¼ ì½ê¸°"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()[:5000]
        except Exception as e:
            return f"[ì˜¤ë¥˜: {e}]"

    @staticmethod
    def write_file(path: str, content: str) -> str:
        """íŒŒì¼ ì“°ê¸°"""
        try:
            with open(path, 'w', encoding='utf-8') as f:
                f.write(content)
            return f"[ì €ì¥: {path}]"
        except Exception as e:
            return f"[ì˜¤ë¥˜: {e}]"

    @staticmethod
    def list_dir(path: str = ".") -> str:
        """ë””ë ‰í† ë¦¬ ëª©ë¡"""
        try:
            files = os.listdir(path)
            return "\n".join(sorted(files)[:50])
        except Exception as e:
            return f"[ì˜¤ë¥˜: {e}]"

    @staticmethod
    def system_info() -> str:
        """ì‹œìŠ¤í…œ ì •ë³´"""
        import platform
        info = [
            f"OS: {platform.system()} {platform.release()}",
            f"Python: {platform.python_version()}",
        ]
        try:
            gpu = subprocess.run(
                "nvidia-smi --query-gpu=name,memory.free --format=csv,noheader",
                shell=True, capture_output=True, text=True, timeout=5
            )
            if gpu.stdout:
                info.append(f"GPU: {gpu.stdout.strip()}")
        except:
            pass
        return "\n".join(info)

    @staticmethod
    def time_now() -> str:
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    @staticmethod
    def calc(expr: str) -> str:
        """ê³„ì‚°"""
        try:
            allowed = set("0123456789+-*/().% ")
            if not all(c in allowed for c in expr):
                return "[í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì]"
            return str(eval(expr))
        except Exception as e:
            return f"[ì˜¤ë¥˜: {e}]"

    @staticmethod
    def web_search(query: str) -> str:
        """ì›¹ ê²€ìƒ‰ (êµ¬í˜„ í•„ìš”)"""
        return f"[ê²€ìƒ‰: {query}] - API í‚¤ í•„ìš”"


class MicListener:
    """ë§ˆì´í¬ ì²­ì·¨"""

    def __init__(self, callback: Callable[[str], None]):
        self.callback = callback
        self.running = False
        self.thread = None

        try:
            import speech_recognition as sr
            self.sr = sr
            self.recognizer = sr.Recognizer()
            self.available = True
        except ImportError:
            self.available = False
            print("âš ï¸ speech_recognition ë¯¸ì„¤ì¹˜")

    def start(self):
        if not self.available:
            return
        self.running = True
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()
        print("ğŸ¤ ë§ˆì´í¬ ì²­ì·¨ ì‹œì‘")

    def stop(self):
        self.running = False

    def _loop(self):
        while self.running:
            try:
                with self.sr.Microphone() as source:
                    self.recognizer.adjust_for_ambient_noise(source, duration=0.3)
                    audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=15)

                try:
                    text = self.recognizer.recognize_google(audio, language="ko-KR")
                    if text:
                        print(f"\nğŸ¤ [{text}]")
                        self.callback(text)
                except self.sr.UnknownValueError:
                    pass
                except self.sr.RequestError:
                    pass
            except:
                time.sleep(0.5)


class OllamaEngine:
    """Ollama ì¶”ë¡  ì—”ì§„"""

    def __init__(self, model: str = OLLAMA_MODEL):
        self.model = model
        self.url = OLLAMA_URL

    def is_available(self) -> bool:
        try:
            r = requests.get(f"{self.url}/api/tags", timeout=3)
            return r.status_code == 200
        except:
            return False

    def generate(self, prompt: str, system: str = None, stream: bool = True) -> str:
        data = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
        }
        if system:
            data["system"] = system

        try:
            if stream:
                r = requests.post(
                    f"{self.url}/api/generate",
                    json=data, stream=True, timeout=60
                )
                response = ""
                for line in r.iter_lines():
                    if line:
                        chunk = json.loads(line)
                        if "response" in chunk:
                            text = chunk["response"]
                            print(text, end="", flush=True)
                            response += text
                        if chunk.get("done"):
                            break
                print()
                return response
            else:
                r = requests.post(f"{self.url}/api/generate", json=data, timeout=60)
                return r.json().get("response", "")
        except Exception as e:
            return f"[Ollama ì˜¤ë¥˜: {e}]"


class HybridAI:
    """í•˜ì´ë¸Œë¦¬ë“œ AI ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.memory = Memory()
        self.memory.load()

        self.tools = Tools()
        self.engine = OllamaEngine()

        self.input_queue = queue.Queue()
        self.mic = None
        self.running = False

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë‹¹ì‹ ì€ HyperCLOVAX ìŠ¤íƒ€ì¼ì˜ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ë„êµ¬ ì‚¬ìš©
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
{{TOOL:tool_name:argument}}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
- bash: ì‰˜ ëª…ë ¹ - {{TOOL:bash:ls -la}}
- python: íŒŒì´ì¬ ì‹¤í–‰ - {{TOOL:python:print(1+1)}}
- read_file: íŒŒì¼ ì½ê¸° - {{TOOL:read_file:/path/to/file}}
- write_file: íŒŒì¼ ì“°ê¸° - {{TOOL:write_file:/path:content}}
- list_dir: ë””ë ‰í† ë¦¬ ëª©ë¡ - {{TOOL:list_dir:/home}}
- system_info: ì‹œìŠ¤í…œ ì •ë³´ - {{TOOL:system_info:}}
- time: í˜„ì¬ ì‹œê°„ - {{TOOL:time:}}
- calc: ê³„ì‚° - {{TOOL:calc:3*5+2}}

## í•™ìŠµ
ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ë©´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì €ì¥:
{{LEARN:category:content}}

ì˜ˆì‹œ:
- {{LEARN:user_preference:ì‚¬ìš©ìëŠ” ê°„ê²°í•œ ë‹µë³€ì„ ì„ í˜¸í•¨}}
- {{LEARN:code_pattern:ì´ í”„ë¡œì íŠ¸ëŠ” Python 3.12 ì‚¬ìš©}}

## ì§€ì¹¨
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”
- í•„ìš”í•˜ë©´ ë„êµ¬ ì‚¬ìš©
- ì¤‘ìš”í•œ ì •ë³´ëŠ” í•™ìŠµìœ¼ë¡œ ì €ì¥
- ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€
"""

    def process_response(self, response: str) -> str:
        """ì‘ë‹µ ì²˜ë¦¬ - ë„êµ¬ ì‹¤í–‰, í•™ìŠµ ì €ì¥"""
        import re

        # ë„êµ¬ ì‹¤í–‰: {{TOOL:name:arg}}
        def exec_tool(match):
            tool_name = match.group(1)
            arg = match.group(2) if match.group(2) else ""

            tool_map = {
                "bash": lambda a: self.tools.bash(a),
                "python": lambda a: self.tools.python(a),
                "read_file": lambda a: self.tools.read_file(a),
                "list_dir": lambda a: self.tools.list_dir(a if a else "."),
                "system_info": lambda a: self.tools.system_info(),
                "time": lambda a: self.tools.time_now(),
                "calc": lambda a: self.tools.calc(a),
            }

            if tool_name in tool_map:
                result = tool_map[tool_name](arg)
                self.memory.log_tool(tool_name, arg, result)
                return f"\nğŸ“Œ [{tool_name}] ê²°ê³¼:\n```\n{result}\n```\n"
            return f"[ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_name}]"

        response = re.sub(r'\{\{TOOL:(\w+):([^}]*)\}\}', exec_tool, response)

        # í•™ìŠµ: {{LEARN:category:content}}
        def save_learn(match):
            category = match.group(1)
            content = match.group(2)
            self.memory.learn(category, content)
            return ""

        response = re.sub(r'\{\{LEARN:(\w+):([^}]*)\}\}', save_learn, response)

        return response

    def build_context(self) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        parts = []

        # ìµœê·¼ í•™ìŠµ
        if self.memory.learnings:
            learns = self.memory.learnings[-10:]
            learn_text = "\n".join([
                f"- [{l['category']}] {l['content']}"
                for l in learns
            ])
            parts.append(f"[í•™ìŠµëœ ì§€ì‹]\n{learn_text}")

        # ìµœê·¼ ëŒ€í™”
        if self.memory.conversations:
            convs = self.memory.conversations[-6:]
            conv_text = "\n".join([
                f"{'User' if c['role']=='user' else 'AI'}: {c['content']}"
                for c in convs
            ])
            parts.append(f"[ëŒ€í™” ê¸°ë¡]\n{conv_text}")

        return "\n\n".join(parts)

    def chat(self, user_input: str) -> str:
        """ëŒ€í™”"""
        self.memory.add_conversation("user", user_input)

        context = self.build_context()
        prompt = f"""{context}

User: {user_input}
AI:"""

        print("\nğŸ¤– ", end="", flush=True)
        response = self.engine.generate(prompt, system=self.system_prompt)

        # ë„êµ¬/í•™ìŠµ ì²˜ë¦¬
        processed = self.process_response(response)
        if processed != response:
            print(processed)

        self.memory.add_conversation("assistant", response)
        self.memory.save()

        return response

    def on_voice(self, text: str):
        """ìŒì„± ì…ë ¥"""
        self.input_queue.put(text)

    def run(self, enable_mic: bool = True):
        """ì‹¤í–‰"""
        print("\n" + "=" * 50)
        print("  ğŸ¤– HyperCLOVAX í•˜ì´ë¸Œë¦¬ë“œ AI")
        print("=" * 50)

        # Ollama í™•ì¸
        if not self.engine.is_available():
            print("âŒ Ollama ë¯¸ì‹¤í–‰. ì‹œì‘: ollama serve")
            return

        print(f"âœ… ëª¨ë¸: {OLLAMA_MODEL}")
        print(f"ğŸ“š í•™ìŠµ: {len(self.memory.learnings)}ê°œ")

        # ë§ˆì´í¬
        if enable_mic:
            self.mic = MicListener(self.on_voice)
            self.mic.start()

        print("\nëª…ë ¹ì–´:")
        print("  /quit      - ì¢…ë£Œ")
        print("  /learn     - í•™ìŠµ ë‚´ìš© ë³´ê¸°")
        print("  /tools     - ìµœê·¼ ë„êµ¬ ì‚¬ìš©")
        print("  /clear     - ëŒ€í™” ì´ˆê¸°í™”")
        print("  /mic on|off - ë§ˆì´í¬ í† ê¸€")
        print("  /teach <ë‚´ìš©> - ì§ì ‘ í•™ìŠµ")
        print("=" * 50 + "\n")

        self.running = True

        while self.running:
            try:
                # ìŒì„± ì…ë ¥ í™•ì¸
                try:
                    text = self.input_queue.get_nowait()
                    self.chat(text)
                    continue
                except queue.Empty:
                    pass

                # í…ìŠ¤íŠ¸ ì…ë ¥
                user_input = input("ğŸ‘¤ You: ").strip()

                if not user_input:
                    continue

                # ëª…ë ¹ì–´
                if user_input.startswith("/"):
                    self._handle_cmd(user_input)
                    continue

                # ëŒ€í™”
                self.chat(user_input)

            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ {e}")

        print("ğŸ‘‹ ì¢…ë£Œ")
        if self.mic:
            self.mic.stop()
        self.memory.save()

    def _handle_cmd(self, cmd: str):
        """ëª…ë ¹ì–´ ì²˜ë¦¬"""
        if cmd == "/quit":
            self.running = False

        elif cmd == "/learn":
            print("\nğŸ“š ìµœê·¼ í•™ìŠµ:")
            for l in self.memory.learnings[-10:]:
                print(f"  [{l['category']}] {l['content'][:60]}")

        elif cmd == "/tools":
            print("\nğŸ”§ ìµœê·¼ ë„êµ¬ ì‚¬ìš©:")
            for t in self.memory.tools_used[-5:]:
                print(f"  {t['tool']}: {t['args'][:40]}")

        elif cmd == "/clear":
            self.memory.conversations = []
            print("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")

        elif cmd == "/mic on":
            if self.mic:
                self.mic.start()

        elif cmd == "/mic off":
            if self.mic:
                self.mic.stop()
                print("ğŸ”‡ ë§ˆì´í¬ ì¤‘ì§€")

        elif cmd.startswith("/teach "):
            content = cmd[7:]
            self.memory.learn("user_taught", content)

        else:
            print("â“ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´")


def main():
    ai = HybridAI()
    ai.run(enable_mic=True)


if __name__ == "__main__":
    main()
