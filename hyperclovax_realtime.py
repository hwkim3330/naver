#!/usr/bin/env python3
"""
HyperCLOVAX ì‹¤ì‹œê°„ AI
- í•­ìƒ ë§ˆì´í¬ ì²­ì·¨
- ë„êµ¬ ì‹¤í–‰
- ìê°€ í•™ìŠµ
- INT4 + CPU Offload ê·¹í•œ ìµœì í™”
"""

import os
import sys
import gc
import json
import time
import queue
import threading
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List, Any, Callable
from dataclasses import dataclass, field
import warnings

warnings.filterwarnings("ignore")

# ê²½ë¡œ ì„¤ì •
MODEL_PATH = "/mnt/data/HyperCLOVAX/model"
sys.path.insert(0, MODEL_PATH)

MEMORY_FILE = "/mnt/data/HyperCLOVAX-AGI/ai_memory.json"


@dataclass
class AIMemory:
    """AI ë©”ëª¨ë¦¬"""
    conversations: List[Dict] = field(default_factory=list)
    learnings: List[Dict] = field(default_factory=list)

    def save(self):
        data = {
            "conversations": self.conversations[-20:],
            "learnings": self.learnings[-100:],
            "updated": datetime.now().isoformat(),
        }
        with open(MEMORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        if os.path.exists(MEMORY_FILE):
            try:
                with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.conversations = data.get("conversations", [])
                self.learnings = data.get("learnings", [])
            except:
                pass

    def add(self, role: str, content: str):
        self.conversations.append({
            "role": role,
            "content": content,
            "time": datetime.now().isoformat()
        })

    def learn(self, content: str):
        self.learnings.append({
            "content": content,
            "time": datetime.now().isoformat()
        })
        print(f"ğŸ’¡ í•™ìŠµ: {content}")


class ToolExecutor:
    """ë„êµ¬ ì‹¤í–‰ê¸°"""

    def run_bash(self, cmd: str) -> str:
        try:
            r = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            return (r.stdout + r.stderr)[:2000] or "[ì™„ë£Œ]"
        except:
            return "[ì˜¤ë¥˜]"

    def run_python(self, code: str) -> str:
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                path = f.name
            r = subprocess.run(['python3', path], capture_output=True, text=True, timeout=30)
            os.unlink(path)
            return (r.stdout + r.stderr)[:2000] or "[ì™„ë£Œ]"
        except:
            return "[ì˜¤ë¥˜]"

    def read_file(self, path: str) -> str:
        try:
            with open(path, 'r') as f:
                return f.read()[:3000]
        except:
            return "[ì˜¤ë¥˜]"

    def write_file(self, path: str, content: str) -> str:
        try:
            with open(path, 'w') as f:
                f.write(content)
            return f"[ì €ì¥: {path}]"
        except:
            return "[ì˜¤ë¥˜]"


class MicListener:
    """ë§ˆì´í¬ ì²­ì·¨"""

    def __init__(self, callback):
        self.callback = callback
        self.running = False
        self.thread = None

        try:
            import speech_recognition as sr
            self.sr = sr
            self.recognizer = sr.Recognizer()
            self.available = True
        except:
            self.available = False

    def start(self):
        if not self.available:
            print("âš ï¸ speech_recognition ë¯¸ì„¤ì¹˜")
            return
        self.running = True
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()
        print("ğŸ¤ ë§ˆì´í¬ ì²­ì·¨ ì‹œì‘")

    def stop(self):
        self.running = False

    def _loop(self):
        while self.running:
            try:
                with self.sr.Microphone() as source:
                    self.recognizer.adjust_for_ambient_noise(source, duration=0.3)
                    audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10)

                try:
                    text = self.recognizer.recognize_google(audio, language="ko-KR")
                    if text:
                        print(f"\nğŸ¤ [{text}]")
                        self.callback(text)
                except:
                    pass
            except:
                time.sleep(0.5)


class HyperCLOVAXRealtime:
    """HyperCLOVAX ì‹¤ì‹œê°„ AI"""

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.memory = AIMemory()
        self.memory.load()
        self.tools = ToolExecutor()
        self.input_queue = queue.Queue()
        self.mic = None
        self.running = False

    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ (ê·¹í•œ ìµœì í™”)"""
        print("\n" + "=" * 50)
        print("ğŸš€ HyperCLOVAX ë¡œë”© (INT4 + CPU Offload)")
        print("=" * 50)

        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

            # CUDA í™•ì¸
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                vram = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"ğŸ–¥ï¸ GPU: {gpu_name} ({vram:.1f}GB)")
            else:
                print("âš ï¸ CUDA ì—†ìŒ - CPU ì „ìš©")

            # í† í¬ë‚˜ì´ì €
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”©...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                MODEL_PATH, trust_remote_code=True
            )

            # INT4 ì–‘ìí™”
            print("âš™ï¸ INT4 ì–‘ìí™” ì„¤ì •...")
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

            # CPU-GPU í•˜ì´ë¸Œë¦¬ë“œ device_map
            print("âš™ï¸ CPU-GPU í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì„±...")

            # ê·¹í•œ ìµœì í™”: ê±°ì˜ ëª¨ë“  ê²ƒì„ CPUì—
            device_map = {
                # ì¸ì½”ë” ì „ë¶€ CPU
                "model.vision_model": "cpu",
                "model.audio_model": "cpu",
                "model.video_audio_compressor": "cpu",

                # ë””ì½”ë” ì „ë¶€ CPU
                "model.discrete_vision_model": "cpu",
                "model.discrete_audio_model": "cpu",

                # Projector - GPU (ì‘ìŒ)
                "model.mm_projector": "cuda:0",
                "model.audio_projector": "cuda:0",

                # LLM - ëŒ€ë¶€ë¶„ CPU, ë§ˆì§€ë§‰ë§Œ GPU
                "model.language_model.model.embed_tokens": "cpu",
                "model.language_model.model.norm": "cuda:0",
                "model.language_model.lm_head": "cuda:0",
            }

            # LLM ë ˆì´ì–´: 32-35ë§Œ GPU, ë‚˜ë¨¸ì§€ CPU
            for i in range(36):
                layer_key = f"model.language_model.model.layers.{i}"
                if i >= 32:  # ë§ˆì§€ë§‰ 4ê°œ ë ˆì´ì–´ë§Œ GPU
                    device_map[layer_key] = "cuda:0"
                else:
                    device_map[layer_key] = "cpu"

            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ”„ ëª¨ë¸ ë¡œë”©... (ìˆ˜ ë¶„ ì†Œìš”)")

            os.makedirs("/tmp/hcx_offload", exist_ok=True)

            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                trust_remote_code=True,
                quantization_config=quant_config,
                device_map=device_map,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                offload_folder="/tmp/hcx_offload",
            )

            # VRAM ìƒíƒœ
            if torch.cuda.is_available():
                used = torch.cuda.memory_allocated() / 1e9
                total = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"ğŸ“Š VRAM: {used:.2f}GB / {total:.2f}GB")

            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def generate(self, prompt: str, max_tokens: int = 256) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if self.model is None:
            return "[ëª¨ë¸ ì—†ìŒ]"

        try:
            import torch

            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512,
            )

            # CPUì— ìˆëŠ” ì„ë² ë”©ìœ¼ë¡œ ì´ë™
            device = "cpu"  # embed_tokensê°€ CPUì— ìˆìŒ
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                )

            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True,
            )

            return response.strip()

        except Exception as e:
            return f"[ìƒì„± ì˜¤ë¥˜: {e}]"

    def process_tools(self, response: str) -> str:
        """ë„êµ¬ ì‹¤í–‰ ì²˜ë¦¬"""
        import re

        # [BASH: cmd] íŒ¨í„´
        def run_bash(m):
            result = self.tools.run_bash(m.group(1))
            return f"\nğŸ“Œ ì‹¤í–‰ ê²°ê³¼:\n{result}\n"
        response = re.sub(r'\[BASH:\s*(.+?)\]', run_bash, response)

        # [PYTHON: code] íŒ¨í„´
        def run_python(m):
            result = self.tools.run_python(m.group(1))
            return f"\nğŸ“Œ ì‹¤í–‰ ê²°ê³¼:\n{result}\n"
        response = re.sub(r'\[PYTHON:\s*(.+?)\]', run_python, response, flags=re.DOTALL)

        # [LEARN: content] íŒ¨í„´
        def learn(m):
            self.memory.learn(m.group(1))
            return ""
        response = re.sub(r'\[LEARN:\s*(.+?)\]', learn, response)

        return response

    def chat(self, user_input: str) -> str:
        """ëŒ€í™”"""
        self.memory.add("user", user_input)

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join([
            f"{'User' if c['role']=='user' else 'AI'}: {c['content']}"
            for c in self.memory.conversations[-5:]
        ])

        # í•™ìŠµ ë‚´ìš©
        learnings = ""
        if self.memory.learnings:
            learnings = "\n[í•™ìŠµëœ ì§€ì‹]\n" + "\n".join([
                f"- {l['content']}" for l in self.memory.learnings[-5:]
            ])

        prompt = f"""ë‹¹ì‹ ì€ HyperCLOVAX AIì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë„êµ¬ ì‚¬ìš©: [BASH: ëª…ë ¹], [PYTHON: ì½”ë“œ], [LEARN: í•™ìŠµë‚´ìš©]

{learnings}

{context}
User: {user_input}
AI:"""

        print("\nğŸ¤– ", end="", flush=True)
        response = self.generate(prompt)
        print(response)

        # ë„êµ¬ ì‹¤í–‰
        processed = self.process_tools(response)
        if processed != response:
            print(processed)

        self.memory.add("assistant", response)
        self.memory.save()

        return response

    def on_voice(self, text: str):
        """ìŒì„± ì…ë ¥"""
        self.input_queue.put(text)

    def run(self, enable_mic: bool = True):
        """ì‹¤í–‰"""
        if not self.load_model():
            return

        print("\n" + "=" * 50)
        print("ğŸ’¬ HyperCLOVAX ëŒ€í™” ëª¨ë“œ")
        print("=" * 50)
        print("ëª…ë ¹: /quit, /learn <ë‚´ìš©>, /status")
        print("=" * 50 + "\n")

        # ë§ˆì´í¬
        if enable_mic:
            self.mic = MicListener(self.on_voice)
            self.mic.start()

        self.running = True

        while self.running:
            try:
                # ìŒì„± ì…ë ¥ í™•ì¸
                try:
                    text = self.input_queue.get_nowait()
                    self.chat(text)
                    continue
                except queue.Empty:
                    pass

                # í…ìŠ¤íŠ¸ ì…ë ¥
                user_input = input("ğŸ‘¤ You: ").strip()

                if not user_input:
                    continue

                if user_input == "/quit":
                    break
                elif user_input.startswith("/learn "):
                    self.memory.learn(user_input[7:])
                elif user_input == "/status":
                    import torch
                    if torch.cuda.is_available():
                        used = torch.cuda.memory_allocated() / 1e9
                        print(f"ğŸ“Š VRAM: {used:.2f}GB")
                    print(f"ğŸ“š í•™ìŠµ: {len(self.memory.learnings)}ê°œ")
                else:
                    self.chat(user_input)

            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ {e}")

        print("ğŸ‘‹ ì¢…ë£Œ")
        if self.mic:
            self.mic.stop()
        self.memory.save()


def main():
    ai = HyperCLOVAXRealtime()
    ai.run(enable_mic=True)


if __name__ == "__main__":
    main()
