#!/usr/bin/env python3
"""
HyperCLOVAX ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- ì´ë¯¸ì§€ ì´í•´
- ì˜¤ë””ì˜¤ ì´í•´
- ë¹„ë””ì˜¤ ì´í•´
- ì´ë¯¸ì§€ ìƒì„±
- ìŒì„± í•©ì„±
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from typing import Optional, List, Dict, Any, Union
from dataclasses import dataclass
from PIL import Image
import warnings

warnings.filterwarnings("ignore")

MODEL_PATH = "/mnt/data/HyperCLOVAX/model"
sys.path.insert(0, MODEL_PATH)


@dataclass
class ModalityInput:
    """ëª¨ë‹¬ë¦¬í‹° ì…ë ¥ ë°ì´í„°"""
    type: str  # "text", "image", "audio", "video"
    data: Any  # ì‹¤ì œ ë°ì´í„°
    path: Optional[str] = None  # íŒŒì¼ ê²½ë¡œ


@dataclass
class ModalityOutput:
    """ëª¨ë‹¬ë¦¬í‹° ì¶œë ¥ ë°ì´í„°"""
    type: str  # "text", "image", "audio"
    data: Any
    metadata: Optional[Dict] = None


class ImageProcessor:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ê¸°"""

    def __init__(self, target_size: int = 448):
        self.target_size = target_size

    def load_image(self, path: str) -> Optional[Image.Image]:
        """ì´ë¯¸ì§€ ë¡œë“œ"""
        try:
            img = Image.open(path).convert("RGB")
            return img
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def preprocess(self, image: Image.Image) -> torch.Tensor:
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # ë¦¬ì‚¬ì´ì¦ˆ
        img = image.resize((self.target_size, self.target_size), Image.LANCZOS)

        # í…ì„œ ë³€í™˜
        img_array = np.array(img).astype(np.float32) / 255.0

        # ì •ê·œí™” (ImageNet ê¸°ì¤€)
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_array = (img_array - mean) / std

        # CHW í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)

        return img_tensor.float()

    def process_anyres(self, image: Image.Image, grid_size: int = 2) -> List[torch.Tensor]:
        """AnyRes ë°©ì‹ ì²˜ë¦¬ (ê³ í•´ìƒë„ ì´ë¯¸ì§€)"""
        w, h = image.size

        # ê¸°ë³¸ ì´ë¯¸ì§€
        base = self.preprocess(image)

        # ê·¸ë¦¬ë“œ ë¶„í• 
        grids = []
        grid_w = w // grid_size
        grid_h = h // grid_size

        for i in range(grid_size):
            for j in range(grid_size):
                crop = image.crop((
                    j * grid_w,
                    i * grid_h,
                    (j + 1) * grid_w,
                    (i + 1) * grid_h
                ))
                grids.append(self.preprocess(crop))

        return [base] + grids


class AudioProcessor:
    """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ê¸°"""

    def __init__(self, sample_rate: int = 16000, n_mels: int = 128):
        self.sample_rate = sample_rate
        self.n_mels = n_mels

    def load_audio(self, path: str) -> Optional[np.ndarray]:
        """ì˜¤ë””ì˜¤ ë¡œë“œ"""
        try:
            import librosa
            audio, sr = librosa.load(path, sr=self.sample_rate)
            return audio
        except ImportError:
            print("âš ï¸ librosa ë¯¸ì„¤ì¹˜. ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë¶ˆê°€")
            return None
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def preprocess(self, audio: np.ndarray) -> torch.Tensor:
        """ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (Mel Spectrogram)"""
        try:
            import librosa

            # Mel spectrogram
            mel = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_mels=self.n_mels,
                hop_length=160,
                n_fft=400,
            )

            # Log scale
            log_mel = librosa.power_to_db(mel, ref=np.max)

            # ì •ê·œí™”
            log_mel = (log_mel + 80) / 80

            # í…ì„œ ë³€í™˜
            return torch.from_numpy(log_mel).unsqueeze(0).float()

        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return torch.zeros(1, self.n_mels, 100)


class VideoProcessor:
    """ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ê¸°"""

    def __init__(self, target_size: int = 224, max_frames: int = 32):
        self.target_size = target_size
        self.max_frames = max_frames
        self.image_processor = ImageProcessor(target_size)

    def load_video(self, path: str) -> Optional[List[Image.Image]]:
        """ë¹„ë””ì˜¤ ë¡œë“œ (í”„ë ˆì„ ì¶”ì¶œ)"""
        try:
            import cv2

            cap = cv2.VideoCapture(path)
            frames = []

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            sample_indices = np.linspace(0, total_frames - 1, self.max_frames, dtype=int)

            for idx in sample_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # BGR to RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(Image.fromarray(frame_rgb))

            cap.release()
            return frames

        except ImportError:
            print("âš ï¸ opencv-python ë¯¸ì„¤ì¹˜. ë¹„ë””ì˜¤ ì²˜ë¦¬ ë¶ˆê°€")
            return None
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def preprocess(self, frames: List[Image.Image]) -> torch.Tensor:
        """ë¹„ë””ì˜¤ í”„ë ˆì„ ì „ì²˜ë¦¬"""
        processed = []
        for frame in frames:
            tensor = self.image_processor.preprocess(frame)
            processed.append(tensor)

        # (batch, frames, channels, height, width)
        return torch.cat(processed, dim=0).unsqueeze(0)


class MultimodalPipeline:
    """ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  íŒŒì´í”„ë¼ì¸"""

    def __init__(self, model, tokenizer, profile):
        self.model = model
        self.tokenizer = tokenizer
        self.profile = profile

        self.image_processor = ImageProcessor()
        self.audio_processor = AudioProcessor()
        self.video_processor = VideoProcessor()

        # íŠ¹ìˆ˜ í† í° ID
        self.IMAGE_PAD = 128062
        self.VIDEO_PAD = 128063
        self.AUDIO_PAD = 128071
        self.EOS_TOKEN = 128001

    def process_image(
        self,
        image_path: str,
        prompt: str = "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ) -> str:
        """ì´ë¯¸ì§€ ì´í•´"""
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        image = self.image_processor.load_image(image_path)
        if image is None:
            return "[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨]"

        pixel_values = self.image_processor.preprocess(image)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"<|IMAGE_PAD|>\n{prompt}"

        try:
            # í† í°í™”
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.profile.max_length,
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            pixel_values = pixel_values.to(device).half()

            # ëª¨ë¸ì´ VLMì¸ ê²½ìš°
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'vision_model'):
                with torch.inference_mode():
                    outputs = self.model.generate(
                        **inputs,
                        pixel_values=[[pixel_values]],
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.7,
                    )
            else:
                # í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸
                with torch.inference_mode():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=256,
                        do_sample=True,
                        temperature=0.7,
                    )

            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True,
            )

            return response.strip()

        except Exception as e:
            return f"[ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}]"

    def process_audio(
        self,
        audio_path: str,
        prompt: str = "ì´ ì˜¤ë””ì˜¤ì˜ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ) -> str:
        """ì˜¤ë””ì˜¤ ì´í•´"""
        # ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
        audio = self.audio_processor.load_audio(audio_path)
        if audio is None:
            return "[ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨]"

        audio_features = self.audio_processor.preprocess(audio)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"<|AUDIO_PAD|>\n{prompt}"

        try:
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.profile.max_length,
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            audio_features = audio_features.to(device).half()

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                )

            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True,
            )

            return response.strip()

        except Exception as e:
            return f"[ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}]"

    def process_video(
        self,
        video_path: str,
        prompt: str = "ì´ ë¹„ë””ì˜¤ì˜ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    ) -> str:
        """ë¹„ë””ì˜¤ ì´í•´"""
        # ë¹„ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬
        frames = self.video_processor.load_video(video_path)
        if frames is None:
            return "[ë¹„ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨]"

        video_tensor = self.video_processor.preprocess(frames)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"<|VIDEO_PAD|>\n{prompt}"

        try:
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.profile.max_length,
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            video_tensor = video_tensor.to(device).half()

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                )

            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True,
            )

            return response.strip()

        except Exception as e:
            return f"[ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}]"

    def generate_image(
        self,
        prompt: str,
        output_path: str = "/tmp/generated_image.png",
    ) -> Optional[str]:
        """ì´ë¯¸ì§€ ìƒì„± (TA-Tok ì‚¬ìš©)"""
        try:
            # ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            gen_prompt = f"<|generate_image|>{prompt}<|endofgenerate|>"

            inputs = self.tokenizer(
                gen_prompt,
                return_tensors="pt",
                truncation=True,
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # ì´ì‚° ë¹„ì „ í† í° ìƒì„±
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,  # ì´ë¯¸ì§€ í† í° ìˆ˜
                    do_sample=True,
                    temperature=0.8,
                )

            # TA-Tok ë””ì½”ë”ë¡œ ì´ë¯¸ì§€ ë³€í™˜
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'discrete_vision_model'):
                # discrete_vision_modelë¡œ í† í° â†’ ì´ë¯¸ì§€ ë³€í™˜
                # ì‹¤ì œ êµ¬í˜„ì€ ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„
                print("âš ï¸ ì´ë¯¸ì§€ ìƒì„±ì€ ë³„ë„ ë””ì½”ë” í•„ìš”")
                return None

            return output_path

        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì˜¤ë¥˜: {e}")
            return None

    def synthesize_speech(
        self,
        text: str,
        output_path: str = "/tmp/synthesized_speech.wav",
    ) -> Optional[str]:
        """ìŒì„± í•©ì„± (CosyVoice2 ì‚¬ìš©)"""
        try:
            # ìŒì„± í•©ì„± í”„ë¡¬í”„íŠ¸
            gen_prompt = f"<|generate_audio|>{text}<|endofgenerate|>"

            inputs = self.tokenizer(
                gen_prompt,
                return_tensors="pt",
                truncation=True,
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # ì´ì‚° ì˜¤ë””ì˜¤ í† í° ìƒì„±
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=2048,  # ì˜¤ë””ì˜¤ í† í° ìˆ˜
                    do_sample=True,
                    temperature=0.8,
                )

            # CosyVoice2 ë””ì½”ë”ë¡œ ì˜¤ë””ì˜¤ ë³€í™˜
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'discrete_audio_model'):
                print("âš ï¸ ìŒì„± í•©ì„±ì€ ë³„ë„ ë””ì½”ë” í•„ìš”")
                return None

            return output_path

        except Exception as e:
            print(f"âŒ ìŒì„± í•©ì„± ì˜¤ë¥˜: {e}")
            return None

    def process(self, inputs: List[ModalityInput], prompt: str) -> ModalityOutput:
        """ë²”ìš© ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬"""
        # ì…ë ¥ ë¶„ì„
        has_image = any(i.type == "image" for i in inputs)
        has_audio = any(i.type == "audio" for i in inputs)
        has_video = any(i.type == "video" for i in inputs)

        # ì ì ˆí•œ ì²˜ë¦¬ê¸° í˜¸ì¶œ
        if has_video:
            video_input = next(i for i in inputs if i.type == "video")
            result = self.process_video(video_input.path, prompt)
        elif has_image:
            image_input = next(i for i in inputs if i.type == "image")
            result = self.process_image(image_input.path, prompt)
        elif has_audio:
            audio_input = next(i for i in inputs if i.type == "audio")
            result = self.process_audio(audio_input.path, prompt)
        else:
            # í…ìŠ¤íŠ¸ ì „ìš©
            result = self._process_text(prompt)

        return ModalityOutput(type="text", data=result)

    def _process_text(self, prompt: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ìš© ì²˜ë¦¬"""
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.profile.max_length,
            )

            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=True,
                    temperature=0.7,
                )

            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True,
            )

            return response.strip()

        except Exception as e:
            return f"[í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}]"


def test_pipeline():
    """íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("=" * 50)
    print("ë©€í‹°ëª¨ë‹¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    from optimized_ai import OptimizedHyperCLOVAX

    # ëª¨ë¸ ë¡œë“œ
    ai = OptimizedHyperCLOVAX()
    if not ai.load():
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = MultimodalPipeline(ai.model, ai.tokenizer, ai.profile)

    # í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ“ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸:")
    result = pipeline._process_text("ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?")
    print(f"ì‘ë‹µ: {result}")

    # ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ (íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
    test_image = "/tmp/test_image.jpg"
    if os.path.exists(test_image):
        print("\nğŸ–¼ï¸ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸:")
        result = pipeline.process_image(test_image, "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?")
        print(f"ì‘ë‹µ: {result}")

    ai.unload()


if __name__ == "__main__":
    test_pipeline()
