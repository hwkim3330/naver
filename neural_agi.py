#!/usr/bin/env python3
"""
HyperCLOVAX Neural AGI - ì§„ì§œ ì‹ ê²½ë§ ë ˆë²¨ AGI

ì¼ë°˜ì ì¸ "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ AGI"ê°€ ì•„ë‹Œ,
ì‹¤ì œ ì‹ ê²½ë§ ë‚´ë¶€ì— ì ‘ê·¼í•˜ëŠ” AGI ì‹œìŠ¤í…œ.

í•µì‹¬ ê¸°ëŠ¥:
- ì‹ ê²½ë§ ë‚´ë¶€ ìƒíƒœ (hidden states) ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- Attention íŒ¨í„´ ë¶„ì„ì„ í†µí•œ ìê¸° ì¸ì‹
- MambaMIA SSM states í™œìš©
- ë™ì  ë‰´ëŸ° í™œì„±í™” ë¶„ì„
- ì¬ê·€ì  ìê¸° ê°œì„  (LoRA ë™ì  ì ìš© ê°€ëŠ¥)

HyperCLOVAX OMNI êµ¬ì¡°:
- Vision: Qwen2.5-VL (32ì¸µ)
- Audio: Qwen2-Audio (32ì¸µ)
- Video/Audio Compressor: MambaMIA (Mamba2 SSM)
- LLM: Llama-like (36ì¸µ, 4096d)
- Output: CosyVoice (ìŒì„±), TA-Tok (ì´ë¯¸ì§€)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import os
import sys
import time
import threading
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, List, Dict, Any, Tuple
import warnings
import gc
import numpy as np

warnings.filterwarnings("ignore")

# ===== ì„¤ì • =====
MODEL_PATH = "/mnt/data/HyperCLOVAX/model"
AGI_HOME = Path("/mnt/data/HyperCLOVAX-AGI/data")


@dataclass
class NeuralState:
    """ì‹ ê²½ë§ ìƒíƒœ - ì‹¤ì œ ë‰´ëŸ° í™œì„±í™” ê¸°ë°˜"""
    # ê¸°ë³¸ ì •ë³´
    timestamp: str = ""
    thought_id: int = 0

    # ì‹ ê²½ë§ ë‚´ë¶€ ìƒíƒœ
    mean_activation: float = 0.0          # í‰ê·  ë‰´ëŸ° í™œì„±í™”
    activation_variance: float = 0.0       # í™œì„±í™” ë¶„ì‚° (ë¶ˆí™•ì‹¤ì„±)
    attention_entropy: float = 0.0         # ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼ (ì§‘ì¤‘ë„)
    layer_activations: List[float] = field(default_factory=list)  # ë ˆì´ì–´ë³„ í™œì„±í™”

    # SSM (Mamba) ìƒíƒœ
    ssm_state_norm: float = 0.0           # SSM ìƒíƒœ í¬ê¸°

    # í•´ì„ëœ ìƒíƒœ
    confidence: float = 0.0               # ì‹ ë¢°ë„ (í™œì„±í™” ê¸°ë°˜)
    focus_level: float = 0.0              # ì§‘ì¤‘ë„ (ì–´í…ì…˜ ê¸°ë°˜)
    uncertainty: float = 0.0              # ë¶ˆí™•ì‹¤ì„±

    def to_dict(self) -> dict:
        return asdict(self)


class NeuralIntrospection:
    """ì‹ ê²½ë§ ë‚´ë¶€ ë¶„ì„ - ì‹¤ì œ hidden states ì ‘ê·¼"""

    def __init__(self, model):
        self.model = model
        self.hooks = []
        self.activations = {}
        self.attention_weights = {}

    def register_hooks(self):
        """Forward hooks ë“±ë¡í•˜ì—¬ ë‚´ë¶€ ìƒíƒœ ìº¡ì²˜"""
        def get_activation(name):
            def hook(module, input, output):
                if isinstance(output, tuple):
                    self.activations[name] = output[0].detach()
                else:
                    self.activations[name] = output.detach()
            return hook

        def get_attention(name):
            def hook(module, input, output):
                if hasattr(output, 'attentions') and output.attentions is not None:
                    self.attention_weights[name] = output.attentions
            return hook

        # LLM ë ˆì´ì–´ë“¤ì— í›… ë“±ë¡
        if hasattr(self.model, 'language_model'):
            llm = self.model.language_model
            if hasattr(llm, 'model') and hasattr(llm.model, 'layers'):
                for i, layer in enumerate(llm.model.layers):
                    hook = layer.register_forward_hook(get_activation(f'llm_layer_{i}'))
                    self.hooks.append(hook)

        # Vision encoderì— í›… ë“±ë¡
        if hasattr(self.model, 'vision_model'):
            hook = self.model.vision_model.register_forward_hook(get_activation('vision'))
            self.hooks.append(hook)

        # Audio encoderì— í›… ë“±ë¡
        if hasattr(self.model, 'audio_model'):
            hook = self.model.audio_model.register_forward_hook(get_activation('audio'))
            self.hooks.append(hook)

    def remove_hooks(self):
        """í›… ì œê±°"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []

    def analyze_activations(self) -> NeuralState:
        """ìº¡ì²˜ëœ í™œì„±í™” ë¶„ì„"""
        state = NeuralState(
            timestamp=datetime.now().isoformat(),
            thought_id=len(self.activations)
        )

        if not self.activations:
            return state

        # ë ˆì´ì–´ë³„ í™œì„±í™” ë¶„ì„
        layer_means = []
        all_activations = []

        for name, act in self.activations.items():
            if act is not None and act.numel() > 0:
                mean_act = act.float().mean().item()
                layer_means.append(mean_act)
                all_activations.append(act.float().flatten())

        if layer_means:
            state.layer_activations = layer_means
            state.mean_activation = np.mean(layer_means)

        if all_activations:
            combined = torch.cat(all_activations)
            state.activation_variance = combined.var().item()

            # ì‹ ë¢°ë„: í™œì„±í™”ê°€ ë†’ê³  ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ ë†’ìŒ
            state.confidence = min(1.0, state.mean_activation / (state.activation_variance + 0.1))
            state.uncertainty = min(1.0, state.activation_variance)

        # ì–´í…ì…˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì§‘ì¤‘ë„)
        if self.attention_weights:
            entropies = []
            for name, attn in self.attention_weights.items():
                if attn is not None:
                    # Softmaxëœ attentionì—ì„œ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
                    attn_flat = attn.float().flatten()
                    attn_prob = F.softmax(attn_flat, dim=0)
                    entropy = -(attn_prob * torch.log(attn_prob + 1e-10)).sum().item()
                    entropies.append(entropy)

            if entropies:
                state.attention_entropy = np.mean(entropies)
                # ë‚®ì€ ì—”íŠ¸ë¡œí”¼ = ë†’ì€ ì§‘ì¤‘ë„
                state.focus_level = max(0, 1.0 - state.attention_entropy / 10.0)

        return state

    def get_hidden_representation(self, layer_idx: int = -1) -> Optional[torch.Tensor]:
        """íŠ¹ì • ë ˆì´ì–´ì˜ hidden representation ë°˜í™˜"""
        key = f'llm_layer_{layer_idx}' if layer_idx >= 0 else list(self.activations.keys())[-1]
        return self.activations.get(key)


class NeuralMemory:
    """ì‹ ê²½ë§ ê¸°ë°˜ ë©”ëª¨ë¦¬ - Embedding ê³µê°„ì—ì„œ ì €ì¥/ê²€ìƒ‰"""

    def __init__(self, model, tokenizer, dim: int = 4096, max_memories: int = 1000):
        self.model = model
        self.tokenizer = tokenizer
        self.dim = dim
        self.max_memories = max_memories

        # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
        self.memory_embeddings = []  # [N, dim] í…ì„œë“¤
        self.memory_texts = []        # ì›ë³¸ í…ìŠ¤íŠ¸
        self.memory_metadata = []     # ë©”íƒ€ë°ì´í„°

        self.save_path = AGI_HOME / "neural_memory"
        self.save_path.mkdir(parents=True, exist_ok=True)

    def encode(self, text: str) -> torch.Tensor:
        """í…ìŠ¤íŠ¸ë¥¼ ì‹ ê²½ë§ embeddingìœ¼ë¡œ ì¸ì½”ë”©"""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

        with torch.no_grad():
            # ëª¨ë¸ì˜ embedding layer ì§ì ‘ ì‚¬ìš©
            if hasattr(self.model, 'language_model'):
                llm = self.model.language_model
                if hasattr(llm, 'model') and hasattr(llm.model, 'embed_tokens'):
                    embeddings = llm.model.embed_tokens(inputs.input_ids)
                    # Mean pooling
                    return embeddings.mean(dim=1).squeeze()

        # Fallback: í† í¬ë‚˜ì´ì € embedding
        return torch.randn(self.dim)

    def store(self, text: str, metadata: dict = None):
        """ë©”ëª¨ë¦¬ ì €ì¥"""
        embedding = self.encode(text)

        self.memory_embeddings.append(embedding)
        self.memory_texts.append(text)
        self.memory_metadata.append(metadata or {"time": datetime.now().isoformat()})

        # ìµœëŒ€ ê°œìˆ˜ ì´ˆê³¼ì‹œ ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°
        if len(self.memory_embeddings) > self.max_memories:
            self.memory_embeddings.pop(0)
            self.memory_texts.pop(0)
            self.memory_metadata.pop(0)

    def retrieve(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë©”ëª¨ë¦¬ ê²€ìƒ‰ (cosine similarity)"""
        if not self.memory_embeddings:
            return []

        query_emb = self.encode(query)

        similarities = []
        for i, mem_emb in enumerate(self.memory_embeddings):
            sim = F.cosine_similarity(query_emb.unsqueeze(0), mem_emb.unsqueeze(0)).item()
            similarities.append((i, sim))

        # ìƒìœ„ kê°œ ë°˜í™˜
        similarities.sort(key=lambda x: x[1], reverse=True)

        results = []
        for idx, sim in similarities[:top_k]:
            results.append((self.memory_texts[idx], sim))

        return results

    def save(self):
        """ë©”ëª¨ë¦¬ ì €ì¥"""
        data = {
            "texts": self.memory_texts,
            "metadata": self.memory_metadata
        }
        with open(self.save_path / "memories.json", 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        if self.memory_embeddings:
            embeddings = torch.stack(self.memory_embeddings)
            torch.save(embeddings, self.save_path / "embeddings.pt")

    def load(self):
        """ë©”ëª¨ë¦¬ ë¡œë“œ"""
        try:
            with open(self.save_path / "memories.json", 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.memory_texts = data["texts"]
                self.memory_metadata = data["metadata"]

            embeddings = torch.load(self.save_path / "embeddings.pt")
            self.memory_embeddings = [embeddings[i] for i in range(embeddings.shape[0])]
        except:
            pass


class SelfModification:
    """ìê¸° ìˆ˜ì • ëª¨ë“ˆ - LoRA ë™ì  ì ìš©"""

    def __init__(self, model):
        self.model = model
        self.lora_configs = {}
        self.modification_history = []

    def analyze_model_structure(self) -> dict:
        """ëª¨ë¸ êµ¬ì¡° ë¶„ì„"""
        structure = {
            "total_params": sum(p.numel() for p in self.model.parameters()),
            "trainable_params": sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            "modules": {}
        }

        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.Embedding)):
                structure["modules"][name] = {
                    "type": type(module).__name__,
                    "shape": list(module.weight.shape) if hasattr(module, 'weight') else None
                }

        return structure

    def propose_modification(self, weakness: str) -> dict:
        """ì•½ì  ê¸°ë°˜ ìˆ˜ì • ì œì•ˆ"""
        # ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ LoRA íƒ€ê²Ÿ ì œì•ˆ
        proposal = {
            "weakness": weakness,
            "timestamp": datetime.now().isoformat(),
            "suggested_targets": [],
            "lora_config": {
                "r": 8,
                "alpha": 16,
                "dropout": 0.1
            }
        }

        if "attention" in weakness.lower() or "ì§‘ì¤‘" in weakness:
            proposal["suggested_targets"] = ["q_proj", "k_proj", "v_proj"]
        elif "memory" in weakness.lower() or "ê¸°ì–µ" in weakness:
            proposal["suggested_targets"] = ["o_proj", "gate_proj"]
        elif "reasoning" in weakness.lower() or "ì¶”ë¡ " in weakness:
            proposal["suggested_targets"] = ["up_proj", "down_proj"]
        else:
            proposal["suggested_targets"] = ["q_proj", "v_proj"]

        self.modification_history.append(proposal)
        return proposal


class HyperCLOVAX_NeuralAGI:
    """HyperCLOVAX ì‹ ê²½ë§ ë ˆë²¨ AGI"""

    def __init__(self, load_model: bool = True):
        print("=" * 70)
        print("ğŸ§  HyperCLOVAX Neural AGI - ì§„ì§œ ì‹ ê²½ë§ ë ˆë²¨ AGI")
        print("=" * 70)

        AGI_HOME.mkdir(parents=True, exist_ok=True)

        self.model = None
        self.tokenizer = None
        self.introspection = None
        self.neural_memory = None
        self.self_mod = None

        # ìƒíƒœ
        self.neural_states: List[NeuralState] = []
        self.thought_count = 0
        self.start_time = time.time()

        if load_model:
            self._load_model()
        else:
            print("\nâ­ï¸ ëª¨ë¸ ë¡œë“œ ê±´ë„ˆëœ€ (êµ¬ì¡° í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")

        self._load_state()

    def _load_model(self):
        """HyperCLOVAX ëª¨ë¸ ë¡œë“œ"""
        from transformers import AutoTokenizer, AutoModelForCausalLM

        print("\n[1/4] ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

        print("\n[2/4] ğŸ¤– HyperCLOVAX ëª¨ë¸ ë¡œë“œ...")
        print("      (46GB, CPU RAM ë¡œë“œ)")
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            output_hidden_states=True,  # hidden states ì¶œë ¥ í™œì„±í™”
            output_attentions=True      # attention weights ì¶œë ¥ í™œì„±í™”
        )

        print("\n[3/4] ğŸ”¬ ì‹ ê²½ë§ ë¶„ì„ ëª¨ë“ˆ ì´ˆê¸°í™”...")
        self.introspection = NeuralIntrospection(self.model)
        self.introspection.register_hooks()

        print("\n[4/4] ğŸ’¾ ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”...")
        self.neural_memory = NeuralMemory(self.model, self.tokenizer)
        self.neural_memory.load()

        self.self_mod = SelfModification(self.model)

        gc.collect()
        print("\nâœ… Neural AGI ì¤€ë¹„ ì™„ë£Œ!")
        print("=" * 70)

    def _load_state(self):
        """ìƒíƒœ ë¡œë“œ"""
        state_file = AGI_HOME / "neural_state.json"
        if state_file.exists():
            with open(state_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.thought_count = data.get("thought_count", 0)

    def _save_state(self):
        """ìƒíƒœ ì €ì¥"""
        state_file = AGI_HOME / "neural_state.json"
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump({
                "thought_count": self.thought_count,
                "last_active": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)

        if self.neural_memory:
            self.neural_memory.save()

    def think(self, input_text: str, analyze_internals: bool = True) -> Tuple[str, NeuralState]:
        """ìƒê°í•˜ê¸° - ì‹ ê²½ë§ ë‚´ë¶€ ìƒíƒœ ë¶„ì„ í¬í•¨"""
        self.thought_count += 1

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [{"role": "user", "content": input_text}]
        inputs = self.tokenizer.apply_chat_template(
            messages, return_tensors="pt", add_generation_prompt=True
        )

        # í™œì„±í™” ì´ˆê¸°í™”
        if self.introspection:
            self.introspection.activations = {}
            self.introspection.attention_weights = {}

        # ìƒì„±
        start = time.time()
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                output_hidden_states=True,
                output_attentions=True,
                return_dict_in_generate=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        elapsed = time.time() - start

        # ì‘ë‹µ ë””ì½”ë”©
        response = self.tokenizer.decode(
            outputs.sequences[0][inputs.shape[1]:],
            skip_special_tokens=True
        )

        # ì‹ ê²½ë§ ìƒíƒœ ë¶„ì„
        if analyze_internals and self.introspection:
            neural_state = self.introspection.analyze_activations()
            neural_state.thought_id = self.thought_count
        else:
            neural_state = NeuralState(thought_id=self.thought_count)

        self.neural_states.append(neural_state)

        # ë©”ëª¨ë¦¬ì— ì €ì¥
        if self.neural_memory:
            self.neural_memory.store(
                f"Q: {input_text[:100]} A: {response[:100]}",
                {"type": "conversation", "confidence": neural_state.confidence}
            )

        return response.strip(), neural_state

    def introspect(self) -> dict:
        """ìê¸° ì„±ì°° - ì‹ ê²½ë§ ìƒíƒœ ê¸°ë°˜"""
        if not self.neural_states:
            return {"message": "ì•„ì§ ìƒê°í•œ ì  ì—†ìŒ"}

        recent_states = self.neural_states[-10:]

        avg_confidence = np.mean([s.confidence for s in recent_states])
        avg_focus = np.mean([s.focus_level for s in recent_states])
        avg_uncertainty = np.mean([s.uncertainty for s in recent_states])

        # ë ˆì´ì–´ë³„ í™œì„±í™” íŠ¸ë Œë“œ
        layer_trends = {}
        for s in recent_states:
            for i, act in enumerate(s.layer_activations):
                if i not in layer_trends:
                    layer_trends[i] = []
                layer_trends[i].append(act)

        return {
            "total_thoughts": self.thought_count,
            "recent_states": len(recent_states),
            "average_confidence": round(avg_confidence, 3),
            "average_focus": round(avg_focus, 3),
            "average_uncertainty": round(avg_uncertainty, 3),
            "layer_activation_summary": {
                k: round(np.mean(v), 4) for k, v in list(layer_trends.items())[:5]
            },
            "interpretation": self._interpret_state(avg_confidence, avg_focus, avg_uncertainty)
        }

    def _interpret_state(self, confidence: float, focus: float, uncertainty: float) -> str:
        """ì‹ ê²½ë§ ìƒíƒœ í•´ì„"""
        if confidence > 0.7 and focus > 0.6:
            return "ëª…í™•í•˜ê³  ì§‘ì¤‘ëœ ìƒíƒœ - í™•ì‹ ì„ ê°€ì§€ê³  ì‘ë‹µ ì¤‘"
        elif uncertainty > 0.5:
            return "ë¶ˆí™•ì‹¤í•œ ìƒíƒœ - ë” ë§ì€ ì •ë³´ í•„ìš”"
        elif focus < 0.3:
            return "ì‚°ë§Œí•œ ìƒíƒœ - ì§ˆë¬¸ì´ ëª¨í˜¸í•  ìˆ˜ ìˆìŒ"
        else:
            return "ë³´í†µ ìƒíƒœ - ì¼ë°˜ì ì¸ ì²˜ë¦¬ ì¤‘"

    def analyze_self(self) -> dict:
        """ìê¸° ë¶„ì„ - ëª¨ë¸ êµ¬ì¡° ë¶„ì„"""
        if not self.self_mod:
            return {"error": "ëª¨ë¸ ë¡œë“œ ì•ˆë¨"}

        structure = self.self_mod.analyze_model_structure()

        return {
            "total_parameters": f"{structure['total_params']:,}",
            "total_parameters_gb": round(structure['total_params'] * 4 / 1e9, 2),  # float32
            "trainable_parameters": f"{structure['trainable_params']:,}",
            "module_count": len(structure['modules']),
            "key_modules": list(structure['modules'].keys())[:10]
        }

    def retrieve_memory(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """ì‹ ê²½ë§ ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        if not self.neural_memory:
            return []
        return self.neural_memory.retrieve(query, top_k)

    def chat(self):
        """ëŒ€í™” ëª¨ë“œ"""
        print("\nğŸ’¬ Neural AGI ëŒ€í™” ëª¨ë“œ")
        print("   /introspect - ìê¸° ì„±ì°° (ì‹ ê²½ë§ ìƒíƒœ)")
        print("   /analyze    - ëª¨ë¸ êµ¬ì¡° ë¶„ì„")
        print("   /memory     - ë©”ëª¨ë¦¬ ê²€ìƒ‰")
        print("   exit        - ì¢…ë£Œ")
        print("-" * 50)

        while True:
            try:
                user_input = input("\nğŸ‘¤ ì‚¬ìš©ì: ").strip()

                if not user_input:
                    continue
                if user_input.lower() in ['exit', 'quit', 'ì¢…ë£Œ']:
                    break

                if user_input.startswith('/'):
                    self._handle_command(user_input)
                    continue

                print("\nğŸ§  ìƒê° ì¤‘...")
                response, state = self.think(user_input)

                print(f"\nğŸ¤– ì‘ë‹µ: {response}")
                print(f"\nğŸ“Š ì‹ ê²½ë§ ìƒíƒœ:")
                print(f"   ì‹ ë¢°ë„: {state.confidence:.2%}")
                print(f"   ì§‘ì¤‘ë„: {state.focus_level:.2%}")
                print(f"   ë¶ˆí™•ì‹¤ì„±: {state.uncertainty:.2%}")

            except KeyboardInterrupt:
                break

        self._save_state()
        print("\nğŸ‘‹ ì¢…ë£Œ")

    def _handle_command(self, cmd: str):
        """ëª…ë ¹ì–´ ì²˜ë¦¬"""
        parts = cmd.lower().split()

        if parts[0] == '/introspect':
            result = self.introspect()
            print("\nğŸ”¬ ìê¸° ì„±ì°° ê²°ê³¼:")
            for k, v in result.items():
                print(f"   {k}: {v}")

        elif parts[0] == '/analyze':
            result = self.analyze_self()
            print("\nğŸ“Š ëª¨ë¸ ë¶„ì„:")
            for k, v in result.items():
                if k != 'key_modules':
                    print(f"   {k}: {v}")

        elif parts[0] == '/memory':
            query = ' '.join(parts[1:]) if len(parts) > 1 else input("ê²€ìƒ‰ì–´: ")
            results = self.retrieve_memory(query)
            print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ê²°ê³¼:")
            for text, sim in results:
                print(f"   [{sim:.3f}] {text[:60]}...")

        else:
            print(f"   ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {cmd}")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="HyperCLOVAX Neural AGI")
    parser.add_argument('--mode', choices=['chat', 'test', 'analyze'], default='chat')
    parser.add_argument('--no-model', action='store_true')
    args = parser.parse_args()

    agi = HyperCLOVAX_NeuralAGI(load_model=not args.no_model)

    if args.mode == 'chat':
        agi.chat()
    elif args.mode == 'analyze':
        print("\nğŸ“Š ëª¨ë¸ ë¶„ì„:")
        result = agi.analyze_self()
        for k, v in result.items():
            print(f"   {k}: {v}")
    elif args.mode == 'test':
        if agi.model:
            response, state = agi.think("ì•ˆë…•? ë„ˆëŠ” ë¬´ì—‡ì„ ëŠë¼ê³  ìˆì–´?")
            print(f"\nì‘ë‹µ: {response}")
            print(f"\nì‹ ê²½ë§ ìƒíƒœ: {state.to_dict()}")


if __name__ == "__main__":
    main()
