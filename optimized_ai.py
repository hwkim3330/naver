#!/usr/bin/env python3
"""
HyperCLOVAX Optimized AI
- INT4/INT8 ì–‘ìí™”
- CPU-GPU í•˜ì´ë¸Œë¦¬ë“œ ë¡œë”©
- ë™ì  VRAM ê´€ë¦¬
- ë©€í‹°ëª¨ë‹¬ ì§€ì›

í™˜ê²½: GTX 1050 Ti 4GB ~ RTX 4090 24GB
"""

import os
import sys
import gc
import json
import torch
import warnings
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from dataclasses import dataclass, field

warnings.filterwarnings("ignore")

# ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = "/mnt/data/HyperCLOVAX/model"

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, MODEL_PATH)


@dataclass
class HardwareProfile:
    """í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼"""
    name: str
    vram_gb: float
    use_quantization: bool = True
    quantization_bits: int = 4
    cpu_offload: bool = True
    max_gpu_layers: int = 8
    batch_size: int = 1
    max_length: int = 512

    # í”„ë¦¬ì…‹
    PROFILES: Dict[str, 'HardwareProfile'] = field(default_factory=dict)


# í•˜ë“œì›¨ì–´ í”„ë¦¬ì…‹
HARDWARE_PRESETS = {
    "gtx1050ti_4gb": HardwareProfile(
        name="GTX 1050 Ti",
        vram_gb=4.0,
        use_quantization=True,
        quantization_bits=4,
        cpu_offload=True,
        max_gpu_layers=4,
        batch_size=1,
        max_length=256,
    ),
    "rtx3060_12gb": HardwareProfile(
        name="RTX 3060",
        vram_gb=12.0,
        use_quantization=True,
        quantization_bits=4,
        cpu_offload=True,
        max_gpu_layers=16,
        batch_size=1,
        max_length=1024,
    ),
    "rtx3090_24gb": HardwareProfile(
        name="RTX 3090",
        vram_gb=24.0,
        use_quantization=True,
        quantization_bits=4,
        cpu_offload=False,
        max_gpu_layers=36,
        batch_size=2,
        max_length=2048,
    ),
    "a100_40gb": HardwareProfile(
        name="A100 40GB",
        vram_gb=40.0,
        use_quantization=False,
        quantization_bits=16,
        cpu_offload=False,
        max_gpu_layers=36,
        batch_size=4,
        max_length=4096,
    ),
    "cpu_only": HardwareProfile(
        name="CPU Only",
        vram_gb=0.0,
        use_quantization=True,
        quantization_bits=8,
        cpu_offload=True,
        max_gpu_layers=0,
        batch_size=1,
        max_length=256,
    ),
}


class VRAMManager:
    """ë™ì  VRAM ê´€ë¦¬ì"""

    def __init__(self, max_vram_gb: float = 4.0):
        self.max_vram_gb = max_vram_gb
        self.max_vram_bytes = int(max_vram_gb * 1024 * 1024 * 1024)

    def get_available_vram(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ VRAM (GB)"""
        if not torch.cuda.is_available():
            return 0.0

        torch.cuda.synchronize()
        allocated = torch.cuda.memory_allocated()
        reserved = torch.cuda.memory_reserved()
        total = torch.cuda.get_device_properties(0).total_memory

        available = total - reserved
        return available / (1024 ** 3)

    def get_used_vram(self) -> float:
        """ì‚¬ìš© ì¤‘ì¸ VRAM (GB)"""
        if not torch.cuda.is_available():
            return 0.0
        return torch.cuda.memory_allocated() / (1024 ** 3)

    def clear_cache(self):
        """VRAM ìºì‹œ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()

    def can_allocate(self, size_gb: float) -> bool:
        """í• ë‹¹ ê°€ëŠ¥ ì—¬ë¶€"""
        return self.get_available_vram() >= size_gb

    def print_status(self):
        """VRAM ìƒíƒœ ì¶œë ¥"""
        if torch.cuda.is_available():
            used = self.get_used_vram()
            available = self.get_available_vram()
            total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
            print(f"ğŸ“Š VRAM: {used:.2f}GB / {total:.2f}GB (ì—¬ìœ : {available:.2f}GB)")
        else:
            print("ğŸ“Š CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")


class DynamicLayerManager:
    """ë™ì  ë ˆì´ì–´ GPU/CPU ê´€ë¦¬"""

    def __init__(self, model, max_gpu_layers: int = 4):
        self.model = model
        self.max_gpu_layers = max_gpu_layers
        self.current_gpu_layers = set()
        self.layer_access_count = {}

    def get_layer(self, layer_idx: int):
        """ë ˆì´ì–´ ê°€ì ¸ì˜¤ê¸° (í•„ìš”ì‹œ GPUë¡œ ì´ë™)"""
        if not hasattr(self.model, 'language_model'):
            return None

        layers = self.model.language_model.model.layers
        if layer_idx >= len(layers):
            return None

        # ì ‘ê·¼ íšŸìˆ˜ ê¸°ë¡
        self.layer_access_count[layer_idx] = self.layer_access_count.get(layer_idx, 0) + 1

        # ì´ë¯¸ GPUì— ìˆìœ¼ë©´ ë°˜í™˜
        if layer_idx in self.current_gpu_layers:
            return layers[layer_idx]

        # GPU ê³µê°„ í™•ë³´
        while len(self.current_gpu_layers) >= self.max_gpu_layers:
            self._evict_least_used()

        # GPUë¡œ ì´ë™
        layers[layer_idx] = layers[layer_idx].to('cuda:0')
        self.current_gpu_layers.add(layer_idx)

        return layers[layer_idx]

    def _evict_least_used(self):
        """ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ë ˆì´ì–´ë¥¼ CPUë¡œ ì´ë™"""
        if not self.current_gpu_layers:
            return

        # ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ë ˆì´ì–´ ì°¾ê¸°
        least_used = min(
            self.current_gpu_layers,
            key=lambda x: self.layer_access_count.get(x, 0)
        )

        # CPUë¡œ ì´ë™
        layers = self.model.language_model.model.layers
        layers[least_used] = layers[least_used].to('cpu')
        self.current_gpu_layers.remove(least_used)

        # ì ‘ê·¼ íšŸìˆ˜ ë¦¬ì…‹
        self.layer_access_count[least_used] = 0

    def move_all_to_cpu(self):
        """ëª¨ë“  ë ˆì´ì–´ë¥¼ CPUë¡œ ì´ë™"""
        if not hasattr(self.model, 'language_model'):
            return

        layers = self.model.language_model.model.layers
        for idx in list(self.current_gpu_layers):
            layers[idx] = layers[idx].to('cpu')
        self.current_gpu_layers.clear()


def detect_hardware() -> HardwareProfile:
    """í•˜ë“œì›¨ì–´ ìë™ ê°ì§€"""
    if not torch.cuda.is_available():
        print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
        return HARDWARE_PRESETS["cpu_only"]

    gpu_name = torch.cuda.get_device_name(0)
    total_vram = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)

    print(f"ğŸ–¥ï¸ GPU: {gpu_name}")
    print(f"ğŸ“Š VRAM: {total_vram:.1f} GB")

    # VRAM ê¸°ë°˜ í”„ë¡œíŒŒì¼ ì„ íƒ
    if total_vram < 6:
        profile = HARDWARE_PRESETS["gtx1050ti_4gb"]
    elif total_vram < 16:
        profile = HARDWARE_PRESETS["rtx3060_12gb"]
    elif total_vram < 32:
        profile = HARDWARE_PRESETS["rtx3090_24gb"]
    else:
        profile = HARDWARE_PRESETS["a100_40gb"]

    # ì‹¤ì œ VRAMìœ¼ë¡œ ì¡°ì •
    profile.vram_gb = total_vram
    print(f"âœ… í”„ë¡œíŒŒì¼: {profile.name}")

    return profile


def create_device_map(profile: HardwareProfile) -> Dict[str, str]:
    """device_map ìƒì„±"""
    device_map = {}

    if profile.cpu_offload or profile.vram_gb < 8:
        # CPU-GPU í•˜ì´ë¸Œë¦¬ë“œ
        device_map = {
            # ì¸ì½”ë”: CPU (VRAM ì ˆì•½)
            "model.vision_model": "cpu",
            "model.audio_model": "cpu",

            # Projector: GPU (ì‘ê³  ë¹ ë¦„)
            "model.mm_projector": "cuda:0" if profile.vram_gb > 2 else "cpu",
            "model.audio_projector": "cuda:0" if profile.vram_gb > 2 else "cpu",

            # MambaMIA: CPU
            "model.video_audio_compressor": "cpu",

            # LLM ì„ë² ë”©: GPU
            "model.language_model.model.embed_tokens": "cuda:0" if profile.vram_gb > 2 else "cpu",

            # LLM ì¶œë ¥: GPU
            "model.language_model.lm_head": "cuda:0" if profile.vram_gb > 2 else "cpu",

            # ë””ì½”ë”: CPU
            "model.discrete_vision_model": "cpu",
            "model.discrete_audio_model": "cpu",
        }

        # LLM ë ˆì´ì–´ ë¶„ë°°
        total_layers = 36
        gpu_layers = min(profile.max_gpu_layers, total_layers)

        for i in range(total_layers):
            layer_key = f"model.language_model.model.layers.{i}"
            if i >= total_layers - gpu_layers:
                device_map[layer_key] = "cuda:0"
            else:
                device_map[layer_key] = "cpu"

        # Norm ë ˆì´ì–´
        device_map["model.language_model.model.norm"] = "cuda:0" if profile.vram_gb > 2 else "cpu"

    else:
        # ì „ì²´ GPU
        device_map = "auto"

    return device_map


def get_quantization_config(profile: HardwareProfile):
    """ì–‘ìí™” ì„¤ì • ìƒì„±"""
    if not profile.use_quantization:
        return None

    from transformers import BitsAndBytesConfig

    if profile.quantization_bits == 4:
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
    elif profile.quantization_bits == 8:
        return BitsAndBytesConfig(
            load_in_8bit=True,
        )

    return None


class OptimizedHyperCLOVAX:
    """ìµœì í™”ëœ HyperCLOVAX ëª¨ë¸"""

    def __init__(
        self,
        model_path: str = MODEL_PATH,
        profile: Optional[HardwareProfile] = None,
        auto_detect: bool = True,
    ):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.vram_manager = None
        self.layer_manager = None

        # í•˜ë“œì›¨ì–´ í”„ë¡œíŒŒì¼
        if profile is None and auto_detect:
            self.profile = detect_hardware()
        elif profile is None:
            self.profile = HARDWARE_PRESETS["cpu_only"]
        else:
            self.profile = profile

        self.vram_manager = VRAMManager(self.profile.vram_gb)

    def load(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        print("\n" + "=" * 50)
        print("ğŸš€ HyperCLOVAX ëª¨ë¸ ë¡œë”©")
        print("=" * 50)

        try:
            from transformers import AutoTokenizer, AutoConfig

            # Config ë¡œë“œ
            print("ğŸ“„ ì„¤ì • ë¡œë“œ ì¤‘...")
            config = AutoConfig.from_pretrained(
                self.model_path,
                trust_remote_code=True,
            )

            # Tokenizer ë¡œë“œ
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
            )

            # ì–‘ìí™” ì„¤ì •
            quant_config = get_quantization_config(self.profile)

            # Device map ìƒì„±
            device_map = create_device_map(self.profile)

            print(f"âš™ï¸ ì–‘ìí™”: {'INT' + str(self.profile.quantization_bits) if self.profile.use_quantization else 'FP16'}")
            print(f"âš™ï¸ CPU Offload: {'í™œì„±í™”' if self.profile.cpu_offload else 'ë¹„í™œì„±í™”'}")

            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

            from transformers import AutoModelForCausalLM

            load_kwargs = {
                "pretrained_model_name_or_path": self.model_path,
                "trust_remote_code": True,
                "torch_dtype": torch.float16,
                "low_cpu_mem_usage": True,
            }

            if quant_config:
                load_kwargs["quantization_config"] = quant_config

            if device_map != "auto":
                load_kwargs["device_map"] = device_map
            else:
                load_kwargs["device_map"] = "auto"

            # offload í´ë” ì„¤ì •
            offload_dir = "/tmp/hcx_offload"
            os.makedirs(offload_dir, exist_ok=True)
            load_kwargs["offload_folder"] = offload_dir

            self.model = AutoModelForCausalLM.from_pretrained(**load_kwargs)

            # ë™ì  ë ˆì´ì–´ ê´€ë¦¬ì ì„¤ì •
            if self.profile.cpu_offload:
                self.layer_manager = DynamicLayerManager(
                    self.model.model if hasattr(self.model, 'model') else self.model,
                    max_gpu_layers=self.profile.max_gpu_layers
                )

            self.vram_manager.print_status()
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

            return True

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def generate(
        self,
        prompt: str,
        max_new_tokens: int = None,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True,
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if self.model is None:
            return "[ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]"

        if max_new_tokens is None:
            max_new_tokens = min(256, self.profile.max_length)

        try:
            # ì…ë ¥ í† í°í™”
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=self.profile.max_length,
            )

            # ë””ë°”ì´ìŠ¤ ì´ë™
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}

            # ìƒì„±
            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.eos_token_id,
                )

            # ë””ì½”ë”©
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True,
            )

            return response.strip()

        except Exception as e:
            return f"[ìƒì„± ì˜¤ë¥˜: {e}]"

    def chat(
        self,
        message: str,
        system_prompt: str = "ë‹¹ì‹ ì€ HyperCLOVAX ê¸°ë°˜ì˜ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
        history: List[Dict[str, str]] = None,
    ) -> str:
        """ëŒ€í™”í˜• ìƒì„±"""
        if history is None:
            history = []

        # ëŒ€í™” í˜•ì‹ êµ¬ì„±
        prompt_parts = [f"System: {system_prompt}\n"]

        for turn in history:
            if turn.get("role") == "user":
                prompt_parts.append(f"User: {turn['content']}\n")
            elif turn.get("role") == "assistant":
                prompt_parts.append(f"Assistant: {turn['content']}\n")

        prompt_parts.append(f"User: {message}\nAssistant:")

        prompt = "".join(prompt_parts)

        return self.generate(prompt)

    def unload(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.layer_manager:
            self.layer_manager.move_all_to_cpu()

        del self.model
        del self.tokenizer

        self.model = None
        self.tokenizer = None

        self.vram_manager.clear_cache()
        print("ğŸ—‘ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


class InteractiveAI:
    """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, model: OptimizedHyperCLOVAX):
        self.model = model
        self.history = []
        self.system_prompt = "ë‹¹ì‹ ì€ HyperCLOVAX ê¸°ë°˜ì˜ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•©ë‹ˆë‹¤."

    def run(self):
        """ëŒ€í™” ë£¨í”„ ì‹¤í–‰"""
        print("\n" + "=" * 50)
        print("ğŸ’¬ HyperCLOVAX ëŒ€í™” ëª¨ë“œ")
        print("=" * 50)
        print("ëª…ë ¹ì–´:")
        print("  /quit - ì¢…ë£Œ")
        print("  /clear - ëŒ€í™” ì´ˆê¸°í™”")
        print("  /system <prompt> - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€ê²½")
        print("  /status - VRAM ìƒíƒœ")
        print("=" * 50 + "\n")

        while True:
            try:
                user_input = input("ğŸ‘¤ You: ").strip()

                if not user_input:
                    continue

                # ëª…ë ¹ì–´ ì²˜ë¦¬
                if user_input.startswith("/"):
                    if user_input == "/quit":
                        print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    elif user_input == "/clear":
                        self.history = []
                        print("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”ë¨")
                        continue
                    elif user_input.startswith("/system "):
                        self.system_prompt = user_input[8:]
                        print(f"âš™ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€ê²½ë¨")
                        continue
                    elif user_input == "/status":
                        self.model.vram_manager.print_status()
                        continue
                    else:
                        print("â“ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´")
                        continue

                # ì‘ë‹µ ìƒì„±
                print("ğŸ¤– AI: ", end="", flush=True)
                response = self.model.chat(
                    message=user_input,
                    system_prompt=self.system_prompt,
                    history=self.history,
                )
                print(response)

                # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self.history.append({"role": "user", "content": user_input})
                self.history.append({"role": "assistant", "content": response})

                # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                if len(self.history) > 10:
                    self.history = self.history[-10:]

            except KeyboardInterrupt:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("  HyperCLOVAX Optimized AI")
    print("  í™˜ê²½ ì ì‘í˜• ë©€í‹°ëª¨ë‹¬ AI")
    print("=" * 60)

    # ëª¨ë¸ ì´ˆê¸°í™”
    ai = OptimizedHyperCLOVAX(
        model_path=MODEL_PATH,
        auto_detect=True,
    )

    # ëª¨ë¸ ë¡œë“œ
    if not ai.load():
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # ëŒ€í™” ëª¨ë“œ ì‹¤í–‰
    interactive = InteractiveAI(ai)
    interactive.run()

    # ì •ë¦¬
    ai.unload()


if __name__ == "__main__":
    main()
